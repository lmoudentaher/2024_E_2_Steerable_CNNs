{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "Baseline CNN data qty variation"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10131922,
          "sourceType": "datasetVersion",
          "datasetId": 6253121
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "acjEmr2xgXw4"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "taherlmouden_equivarient_path = kagglehub.dataset_download('taherlmouden/equivarient')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Qjg4UrTXgXw7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcPqq5YWxN_a",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Changer le répertoire actuel\n",
        "os.chdir('/kaggle/input/equivarient/e2cnn-master')\n",
        "\n",
        "# Vérifier le répertoire actuel\n",
        "print(\"Répertoire actuel :\", os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqqtyW0B0s1Y",
        "outputId": "a71bee17-51bc-4d9f-d46d-449515a68cf7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:35.942171Z",
          "iopub.execute_input": "2024-12-08T13:38:35.942538Z",
          "iopub.status.idle": "2024-12-08T13:38:35.970027Z",
          "shell.execute_reply.started": "2024-12-08T13:38:35.942486Z",
          "shell.execute_reply": "2024-12-08T13:38:35.969165Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Répertoire actuel : /kaggle/input/equivarient/e2cnn-master\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General E(2)-Equivariant Steerable CNNs  -  A concrete example\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BLUZNUc1xADS",
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from e2cnn import gspaces\n",
        "from e2cnn import nn"
      ],
      "metadata": {
        "id": "0UllWhFexADU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:35.971619Z",
          "iopub.execute_input": "2024-12-08T13:38:35.972372Z",
          "iopub.status.idle": "2024-12-08T13:38:40.186082Z",
          "shell.execute_reply.started": "2024-12-08T13:38:35.972333Z",
          "shell.execute_reply": "2024-12-08T13:38:40.185384Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we build a **Steerable CNN** and try it MNIST.\n",
        "\n",
        "Let's also use a group a bit larger: we now build a model equivariant to $8$ rotations.\n",
        "We indicate the group of $N$ discrete rotations as $C_N$, i.e. the **cyclic group** of order $N$.\n",
        "In this case, we will use $C_8$.\n",
        "\n",
        "Because the inputs are still gray-scale images, the input type of the model is again a *scalar field*.\n",
        "\n",
        "However, internally we use *regular fields*: this is equivalent to a *group-equivariant convolutional neural network*.\n",
        "\n",
        "Finally, we build *invariant* features for the final classification task by pooling over the group using *Group Pooling*.\n",
        "\n",
        "The final classification is performed by a two fully connected layers."
      ],
      "metadata": {
        "id": "1OlgTw1rxADV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The model\n",
        "\n",
        "Here is the definition of our model:"
      ],
      "metadata": {
        "id": "yE929vNbxADW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NormalCNN(nn.Module):\n",
        "    def __init__(self, n_classes=10):\n",
        "        super(NormalCNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 24, kernel_size=7, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(24, 48, kernel_size=5, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(48, 48, kernel_size=5, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(48, 96, kernel_size=5, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(96, 96, kernel_size=5, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(96, 64, kernel_size=5, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=1, stride=1)\n",
        "        )\n",
        "\n",
        "        # Dynamically calculate flattened size\n",
        "        dummy_input = torch.zeros(1, 1, 28, 28)  # Adjust input size if needed\n",
        "        dummy_output = self.conv6(self.conv5(self.conv4(self.conv3(self.conv2(self.conv1(dummy_input))))))\n",
        "        flattened_size = dummy_output.numel()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(flattened_size, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.Linear(64, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "aT1l8HSgxADW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:40.187107Z",
          "iopub.execute_input": "2024-12-08T13:38:40.187442Z",
          "iopub.status.idle": "2024-12-08T13:38:40.197982Z",
          "shell.execute_reply.started": "2024-12-08T13:38:40.187416Z",
          "shell.execute_reply": "2024-12-08T13:38:40.197069Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the model on *rotated* MNIST"
      ],
      "metadata": {
        "id": "7cvZVi62xADW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.transforms import Compose, Pad, ToTensor, Normalize, ToPILImage\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RotatedMNISTDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "        This class provides MNIST images with random rotations sampled from\n",
        "        a list of rotation angles. This list is dependent of the number of tasks\n",
        "        `num_tasks` and the distance (measured in degrees) between tasks\n",
        "        `per_task_rotation`.\n",
        "    '''\n",
        "    def __init__(self, root, train=True, transform=None, download=True, num_tasks=5, per_task_rotation=45):\n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, transform=transform, download=download)\n",
        "        self.transform = transform\n",
        "        self.rotation_angles = []\n",
        "        for task in range(num_tasks):\n",
        "            self.rotation_angles.append(float((task) * per_task_rotation))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        angle = np.random.choice(self.rotation_angles)  # Randomly choose a rotation angle\n",
        "        rotated_image = F.rotate(image, angle, fill=(0,))\n",
        "\n",
        "\n",
        "        return rotated_image, label, angle\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:40.199858Z",
          "iopub.execute_input": "2024-12-08T13:38:40.200101Z",
          "iopub.status.idle": "2024-12-08T13:38:41.072424Z",
          "shell.execute_reply.started": "2024-12-08T13:38:40.200077Z",
          "shell.execute_reply": "2024-12-08T13:38:41.071541Z"
        },
        "id": "8Gj8z4k3gXxA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def flattened_rotMNIST(num_tasks,\n",
        "                       per_task_rotation,\n",
        "                       batch_size,\n",
        "                       transform=[],\n",
        "                       ):\n",
        "    '''\n",
        "    Returns:\n",
        "    - train_loader\n",
        "    - test_loader\n",
        "    '''\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(0)  # Ensure consistent ordering across runs\n",
        "\n",
        "    # Extend the provided transform with default Pad, ToTensor, and Normalize\n",
        "    extended_transform = transform.copy()\n",
        "    extended_transform.extend([\n",
        "        Pad((0, 0, 1, 1)),  # Add padding\n",
        "        ToTensor(),\n",
        "        Normalize((0.1307,), (0.3081,))  # Normalize\n",
        "    ])\n",
        "    transforms = Compose(extended_transform)\n",
        "\n",
        "    # Create train and test datasets\n",
        "    train = RotatedMNISTDataset(\n",
        "        root='~/data/', train=True, download=True,\n",
        "        transform=transforms, num_tasks=num_tasks, per_task_rotation=per_task_rotation\n",
        "    )\n",
        "    test = RotatedMNISTDataset(\n",
        "        root='~/data/', train=False, download=True,\n",
        "        transform=transforms, num_tasks=num_tasks, per_task_rotation=per_task_rotation\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=0, pin_memory=True, generator=g\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=0, pin_memory=True, generator=g\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:41.073382Z",
          "iopub.execute_input": "2024-12-08T13:38:41.073745Z",
          "iopub.status.idle": "2024-12-08T13:38:41.080509Z",
          "shell.execute_reply.started": "2024-12-08T13:38:41.073716Z",
          "shell.execute_reply": "2024-12-08T13:38:41.079699Z"
        },
        "id": "q3leNSzxgXxB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary modules\n",
        "import torch\n",
        "\n",
        "# Step 2: Set parameters for the new dataset\n",
        "num_tasks = 8  # Number of rotation tasks\n",
        "per_task_rotation = 45  # Degrees of rotation per task\n",
        "batch_size = 64  # Batch size for training/testing\n",
        "\n",
        "# Step 3: Load the rotated MNIST dataset\n",
        "train_loader, test_loader = flattened_rotMNIST(\n",
        "    num_tasks=num_tasks,\n",
        "    per_task_rotation=per_task_rotation,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "for images, labels, angles in train_loader:\n",
        "    print(f\"Input image shape after transform: {images.shape}\")  # Should be [batch_size, 1, 29, 29]\n",
        "    break\n"
      ],
      "metadata": {
        "id": "v2yOYDTgxADW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e29815-602c-4d19-a57c-2fc1b563f226",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:41.081589Z",
          "iopub.execute_input": "2024-12-08T13:38:41.081839Z",
          "iopub.status.idle": "2024-12-08T13:38:46.378893Z",
          "shell.execute_reply.started": "2024-12-08T13:38:41.081814Z",
          "shell.execute_reply": "2024-12-08T13:38:46.377989Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 9912422/9912422 [00:00<00:00, 17732524.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting /root/data/MNIST/raw/train-images-idx3-ubyte.gz to /root/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 28881/28881 [00:00<00:00, 468891.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting /root/data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1648877/1648877 [00:00<00:00, 4415798.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting /root/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 4542/4542 [00:00<00:00, 3728087.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting /root/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/data/MNIST/raw\n\nInput image shape after transform: torch.Size([64, 1, 29, 29])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import RandomRotation\n",
        "from torchvision.transforms import Pad\n",
        "from torchvision.transforms import Resize\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "bfQdxQa5xADX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:46.38Z",
          "iopub.execute_input": "2024-12-08T13:38:46.380281Z",
          "iopub.status.idle": "2024-12-08T13:38:46.385016Z",
          "shell.execute_reply.started": "2024-12-08T13:38:46.380255Z",
          "shell.execute_reply": "2024-12-08T13:38:46.383827Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the dataset"
      ],
      "metadata": {
        "id": "Tp027s2axADX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistRotDataset(Dataset):\n",
        "\n",
        "    def __init__(self, mode, transform=None):\n",
        "        assert mode in ['train', 'test']\n",
        "\n",
        "        if mode == \"train\":\n",
        "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_train_valid.amat\"\n",
        "        else:\n",
        "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_test.amat\"\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        data = np.loadtxt(file, delimiter=' ')\n",
        "\n",
        "        self.images = data[:, :-1].reshape(-1, 28, 28).astype(np.float32)\n",
        "        self.labels = data[:, -1].astype(np.int64)\n",
        "        self.num_samples = len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.images[index], self.labels[index]\n",
        "        image = Image.fromarray(image)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# images are padded to have shape 29x29.\n",
        "# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\n",
        "pad = Pad((0, 0, 1, 1), fill=0)\n",
        "\n",
        "# to reduce interpolation artifacts (e.g. when testing the model on rotated images),\n",
        "# we upsample an image by a factor of 3, rotate it and finally downsample it again\n",
        "resize1 = Resize(87)\n",
        "resize2 = Resize(29)\n",
        "\n",
        "totensor = ToTensor()"
      ],
      "metadata": {
        "id": "2bwUWP55xADX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:46.386169Z",
          "iopub.execute_input": "2024-12-08T13:38:46.386799Z",
          "iopub.status.idle": "2024-12-08T13:38:46.409737Z",
          "shell.execute_reply.started": "2024-12-08T13:38:46.386765Z",
          "shell.execute_reply": "2024-12-08T13:38:46.408879Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the model"
      ],
      "metadata": {
        "id": "3V7y7Y7PxADY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_2aTGcrxADY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is now randomly initialized.\n",
        "Therefore, we do not expect it to produce the right class probabilities.\n",
        "\n",
        "However, the model should still produce the same output for rotated versions of the same image.\n",
        "This is true for rotations by multiples of $\\frac{\\pi}{2}$, but is only approximate for rotations by $\\frac{\\pi}{4}$.\n",
        "\n",
        "Let's test it on a random test image:\n",
        "we feed eight rotated versions of the first image in the test set and print the output logits of the model for each of them."
      ],
      "metadata": {
        "id": "CgY_kvapxADY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_model(model: torch.nn.Module, x: Image):\n",
        "    # evaluate the `model` on 8 rotated versions of the input image `x`\n",
        "    model.eval()\n",
        "\n",
        "    wrmup = model(torch.randn(1, 1, 29, 29).to(device))\n",
        "    del wrmup\n",
        "\n",
        "    x = resize1(pad(x))\n",
        "\n",
        "    print()\n",
        "    print('##########################################################################################')\n",
        "    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(10)])\n",
        "    print(header)\n",
        "    with torch.no_grad():\n",
        "        for r in range(8):\n",
        "            x_transformed = totensor(resize2(x.rotate(r*45., Image.BILINEAR))).reshape(1, 1, 29, 29)\n",
        "            x_transformed = x_transformed.to(device)\n",
        "\n",
        "            y = model(x_transformed)\n",
        "            y = y.to('cpu').numpy().squeeze()\n",
        "\n",
        "            angle = r * 45\n",
        "            print(\"{:5d} : {}\".format(angle, y))\n",
        "    print('##########################################################################################')\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "Aj4uO72RxADY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:46.410769Z",
          "iopub.execute_input": "2024-12-08T13:38:46.411081Z",
          "iopub.status.idle": "2024-12-08T13:38:46.421974Z",
          "shell.execute_reply.started": "2024-12-08T13:38:46.411046Z",
          "shell.execute_reply": "2024-12-08T13:38:46.421065Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsPbTf7OxADY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQTXgSa0xADY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the model is already almost invariant.\n",
        "However, we still observe small fluctuations in the outputs.\n",
        "\n",
        "This is because the model contains some operations which might break equivariance.\n",
        "For instance, every convolution includes a padding of $2$ pixels per side. This is adds information about the actual orientation of the grid where the image/feature map is sampled because the padding is not rotated with the image.\n",
        "\n",
        "During training, the model will observe rotated patterns and will learn to ignore the noise coming from the padding."
      ],
      "metadata": {
        "id": "EfuBMCFPxADY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, let's train the model now.\n",
        "The model is exactly the same used to train a normal *PyTorch* architecture:"
      ],
      "metadata": {
        "id": "6WxgqeXhxADY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Vm3B3UAcxADY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:38:46.424049Z",
          "iopub.execute_input": "2024-12-08T13:38:46.424302Z",
          "iopub.status.idle": "2024-12-08T13:38:46.434817Z",
          "shell.execute_reply.started": "2024-12-08T13:38:46.424278Z",
          "shell.execute_reply": "2024-12-08T13:38:46.433967Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "# Create or open a CSV file\n",
        "csv_file = \"/kaggle/working/accuracy_by_percentage.csv\"\n",
        "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Data Percentage\", \"Num Images\", \"Epoch\", \"Accuracy\"])  # CSV header\n",
        "\n",
        "# Define percentages to sample\n",
        "percentages = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "# Function to create a subset DataLoader for a specific percentage\n",
        "def get_subset_loader(full_loader, percentage):\n",
        "    # Get all indices and labels from the existing DataLoader\n",
        "    indices = []\n",
        "    labels = []\n",
        "    for i, (x, t, _) in enumerate(full_loader.dataset):\n",
        "        indices.append(i)\n",
        "        labels.append(t)\n",
        "\n",
        "    indices = np.array(indices)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Split indices by label\n",
        "    subset_indices = []\n",
        "    for label in np.unique(labels):\n",
        "        label_indices = indices[labels == label]\n",
        "        num_samples = int(len(label_indices) * (percentage / 100))\n",
        "        subset_indices.extend(label_indices[:num_samples])\n",
        "\n",
        "    # Create a subset DataLoader\n",
        "    subset_dataset = Subset(full_loader.dataset, subset_indices)\n",
        "    subset_loader = DataLoader(subset_dataset, batch_size=full_loader.batch_size, shuffle=True)\n",
        "    return subset_loader, len(subset_indices)\n",
        "\n",
        "# Train the model for different data percentages\n",
        "for percentage in percentages:\n",
        "\n",
        "    model = NormalCNN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "\n",
        "    # Get the subset DataLoader\n",
        "    subset_loader, num_images = get_subset_loader(train_loader, percentage)\n",
        "    print(f\"Training with {percentage}% of the data ({num_images} images)\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(11):  # Number of epochs\n",
        "        model.train()\n",
        "        for i, (x, t, _) in enumerate(subset_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x.to(device)\n",
        "            t = t.to(device)\n",
        "\n",
        "            y = model(x)\n",
        "            loss = loss_function(y, t)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Testing loop\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for i, (x, t, _) in enumerate(test_loader):\n",
        "                x = x.to(device)\n",
        "                t = t.to(device)\n",
        "\n",
        "                y = model(x)\n",
        "\n",
        "                _, prediction = torch.max(y.data, 1)\n",
        "                total += t.shape[0]\n",
        "                correct += (prediction == t).sum().item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        test_accuracy = correct / total * 100.0\n",
        "        print(f\"Percentage {percentage}% | Epoch {epoch} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "        # Save results to the CSV file\n",
        "        with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([percentage, num_images, epoch, test_accuracy])\n"
      ],
      "metadata": {
        "id": "Bmr1gRl6xADY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c62f661-e490-4569-baf1-2c47a3d1785a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:41:04.762924Z",
          "iopub.execute_input": "2024-12-08T13:41:04.763253Z",
          "iopub.status.idle": "2024-12-08T13:41:05.69881Z",
          "shell.execute_reply.started": "2024-12-08T13:41:04.763223Z",
          "shell.execute_reply": "2024-12-08T13:41:05.697533Z"
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Get the subset DataLoader\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m subset_loader, num_images \u001b[38;5;241m=\u001b[39m \u001b[43mget_subset_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% of the data (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mget_subset_loader\u001b[0;34m(full_loader, percentage)\u001b[0m\n\u001b[1;32m     18\u001b[0m indices \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, t, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(full_loader\u001b[38;5;241m.\u001b[39mdataset):\n\u001b[1;32m     21\u001b[0m     indices\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     22\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(t)\n",
            "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mRotatedMNISTDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m image, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx]\n\u001b[1;32m     29\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_angles)  \u001b[38;5;66;03m# Randomly choose a rotation angle\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m rotated_image \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rotated_image, label, angle\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1132\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;66;03m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# we need to set -angle.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m matrix \u001b[38;5;241m=\u001b[39m _get_inverse_affine_matrix(center_f, \u001b[38;5;241m-\u001b[39mangle, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], \u001b[38;5;241m1.0\u001b[39m, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m])\n\u001b[0;32m-> 1132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:665\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[1;32m    663\u001b[0m theta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(matrix, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[0;32m--> 665\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43m_gen_affine_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_grid_transform(img, grid, interpolation, fill\u001b[38;5;241m=\u001b[39mfill)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:592\u001b[0m, in \u001b[0;36m_gen_affine_grid\u001b[0;34m(theta, w, h, ow, oh)\u001b[0m\n\u001b[1;32m    590\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    591\u001b[0m base_grid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m1\u001b[39m, oh, ow, \u001b[38;5;241m3\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtheta\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtheta\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 592\u001b[0m x_grid \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m base_grid[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy_(x_grid)\n\u001b[1;32m    594\u001b[0m y_grid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39moh \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m d, oh \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m d \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, steps\u001b[38;5;241m=\u001b[39moh, device\u001b[38;5;241m=\u001b[39mtheta\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to display a batch of images\n",
        "def show_images_from_loader(loader, classes, num_images=8):\n",
        "    \"\"\"\n",
        "    Displays a batch of images from the given DataLoader.\n",
        "\n",
        "    Args:\n",
        "    - loader: The DataLoader to pull images from (e.g., test_loader).\n",
        "    - classes: A list of class names corresponding to the dataset labels.\n",
        "    - num_images: The number of images to display (default is 8).\n",
        "    \"\"\"\n",
        "    # Get a single batch from the loader\n",
        "    data_iter = iter(loader)\n",
        "    images, labels, _ = next(data_iter)\n",
        "\n",
        "    # Limit to the specified number of images\n",
        "    images = images[:num_images]\n",
        "    labels = labels[:num_images]\n",
        "\n",
        "    # Convert from torch tensors to numpy for visualization\n",
        "    images = images.numpy()\n",
        "\n",
        "    # Create a figure\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        ax = axes[i]\n",
        "        img = np.transpose(images[i], (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
        "        ax.imshow(img)\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(classes[labels[i]])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming you have a DataLoader named `test_loader` and a `classes` list\n",
        "# For example: classes = ['cat', 'dog', 'bird', ...] based on your dataset\n",
        "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']  # Replace with your actual class names\n",
        "show_images_from_loader(test_loader, classes)\n"
      ],
      "metadata": {
        "id": "7dboU8EvDD1p",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T13:41:04.012221Z",
          "iopub.status.idle": "2024-12-08T13:41:04.012545Z",
          "shell.execute_reply.started": "2024-12-08T13:41:04.012386Z",
          "shell.execute_reply": "2024-12-08T13:41:04.012401Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}