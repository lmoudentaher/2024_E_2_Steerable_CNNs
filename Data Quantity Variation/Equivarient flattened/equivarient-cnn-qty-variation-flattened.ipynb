{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10131922,"sourceType":"datasetVersion","datasetId":6253121}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n\n# Changer le répertoire actuel\nos.chdir('/kaggle/input/equivarient/e2cnn-master')\n\n# Vérifier le répertoire actuel\nprint(\"Répertoire actuel :\", os.getcwd())\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqqtyW0B0s1Y","outputId":"55777b3c-93d8-4afc-9952-e2ce0653e90f","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:15:52.597499Z","iopub.execute_input":"2024-12-08T13:15:52.597769Z","iopub.status.idle":"2024-12-08T13:15:52.631801Z","shell.execute_reply.started":"2024-12-08T13:15:52.597741Z","shell.execute_reply":"2024-12-08T13:15:52.630883Z"}},"outputs":[{"name":"stdout","text":"Répertoire actuel : /kaggle/input/equivarient/e2cnn-master\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# General E(2)-Equivariant Steerable CNNs  -  A concrete example\n","metadata":{"collapsed":true,"id":"BLUZNUc1xADS","jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"import torch\n\nfrom e2cnn import gspaces\nfrom e2cnn import nn","metadata":{"id":"0UllWhFexADU","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:15:52.633384Z","iopub.execute_input":"2024-12-08T13:15:52.633738Z","iopub.status.idle":"2024-12-08T13:15:58.032325Z","shell.execute_reply.started":"2024-12-08T13:15:52.633697Z","shell.execute_reply":"2024-12-08T13:15:58.031397Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Finally, we build a **Steerable CNN** and try it MNIST.\n\nLet's also use a group a bit larger: we now build a model equivariant to $8$ rotations.\nWe indicate the group of $N$ discrete rotations as $C_N$, i.e. the **cyclic group** of order $N$.\nIn this case, we will use $C_8$.\n\nBecause the inputs are still gray-scale images, the input type of the model is again a *scalar field*.\n\nHowever, internally we use *regular fields*: this is equivalent to a *group-equivariant convolutional neural network*.\n\nFinally, we build *invariant* features for the final classification task by pooling over the group using *Group Pooling*.\n\nThe final classification is performed by a two fully connected layers.","metadata":{"id":"1OlgTw1rxADV"}},{"cell_type":"markdown","source":"# The model\n\nHere is the definition of our model:","metadata":{"id":"yE929vNbxADW"}},{"cell_type":"code","source":"class C8SteerableCNN(torch.nn.Module):\n\n    def __init__(self, n_classes=10):\n\n        super(C8SteerableCNN, self).__init__()\n\n        self.mask = torch.ones(1, 1, 28, 28)  # Adjust the shape to match your input dimensions\n        self.mask = self.mask.to(device)\n\n        # the model is equivariant under rotations by 45 degrees, modelled by C8\n        self.r2_act = gspaces.Rot2dOnR2(N=8)\n\n        # the input image is a scalar field, corresponding to the trivial representation\n        in_type = nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n\n        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n        self.input_type = in_type\n\n        # convolution 1\n        # first specify the output type of the convolutional layer\n        # we choose 24 feature fields, each transforming under the regular representation of C8\n        out_type = nn.FieldType(self.r2_act, 24*[self.r2_act.regular_repr])\n        self.block1 = nn.SequentialModule(\n            nn.MaskModule(in_type, 29, margin=1),\n            nn.R2Conv(in_type, out_type, kernel_size=7, padding=1, bias=False),\n            nn.InnerBatchNorm(out_type),\n            nn.ReLU(out_type, inplace=True)\n        )\n\n        # convolution 2\n        # the old output type is the input type to the next layer\n        in_type = self.block1.out_type\n        # the output type of the second convolution layer are 48 regular feature fields of C8\n        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n        self.block2 = nn.SequentialModule(\n            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n            nn.InnerBatchNorm(out_type),\n            nn.ReLU(out_type, inplace=True)\n        )\n        self.pool1 = nn.SequentialModule(\n            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n        )\n\n        # convolution 3\n        # the old output type is the input type to the next layer\n        in_type = self.block2.out_type\n        # the output type of the third convolution layer are 48 regular feature fields of C8\n        out_type = nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n        self.block3 = nn.SequentialModule(\n            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n            nn.InnerBatchNorm(out_type),\n            nn.ReLU(out_type, inplace=True)\n        )\n\n        # convolution 4\n        # the old output type is the input type to the next layer\n        in_type = self.block3.out_type\n        # the output type of the fourth convolution layer are 96 regular feature fields of C8\n        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n        self.block4 = nn.SequentialModule(\n            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n            nn.InnerBatchNorm(out_type),\n            nn.ReLU(out_type, inplace=True)\n        )\n        self.pool2 = nn.SequentialModule(\n            nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n        )\n\n        # convolution 5\n        # the old output type is the input type to the next layer\n        in_type = self.block4.out_type\n        # the output type of the fifth convolution layer are 96 regular feature fields of C8\n        out_type = nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n        self.block5 = nn.SequentialModule(\n            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n            nn.InnerBatchNorm(out_type),\n            nn.ReLU(out_type, inplace=True)\n        )\n\n        # convolution 6\n        # the old output type is the input type to the next layer\n        in_type = self.block5.out_type\n        # the output type of the sixth convolution layer are 64 regular feature fields of C8\n        out_type = nn.FieldType(self.r2_act, 64*[self.r2_act.regular_repr])\n        self.block6 = nn.SequentialModule(\n            nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False),\n            nn.InnerBatchNorm(out_type),\n            nn.ReLU(out_type, inplace=True)\n        )\n        self.pool3 = nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=1, padding=0)\n\n        self.gpool = nn.GroupPooling(out_type)\n\n        # number of output channels\n        c = self.gpool.out_type.size\n\n        # Fully Connected\n        self.fully_net = torch.nn.Sequential(\n            torch.nn.Linear(c, 64),\n            torch.nn.BatchNorm1d(64),\n            torch.nn.ELU(inplace=True),\n            torch.nn.Linear(64, n_classes),\n        )\n\n    def forward(self, input: torch.Tensor):\n        # wrap the input tensor in a GeometricTensor\n        # (associate it with the input type)\n        x = nn.GeometricTensor(input, self.input_type)\n\n        # apply each equivariant block\n\n        # Each layer has an input and an output type\n        # A layer takes a GeometricTensor in input.\n        # This tensor needs to be associated with the same representation of the layer's input type\n        #\n        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n        # As a result, consecutive layers need to have matching input/output types\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool1(x)\n\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.pool2(x)\n\n        x = self.block5(x)\n        x = self.block6(x)\n\n        # pool over the spatial dimensions\n        x = self.pool3(x)\n\n        # pool over the group\n        x = self.gpool(x)\n\n        # unwrap the output GeometricTensor\n        # (take the Pytorch tensor and discard the associated representation)\n        x = x.tensor\n\n        # classify with the final fully connected layers)\n        x = self.fully_net(x.reshape(x.shape[0], -1))\n\n        return x","metadata":{"id":"aT1l8HSgxADW","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:15:58.033892Z","iopub.execute_input":"2024-12-08T13:15:58.034255Z","iopub.status.idle":"2024-12-08T13:15:58.049858Z","shell.execute_reply.started":"2024-12-08T13:15:58.034229Z","shell.execute_reply":"2024-12-08T13:15:58.048904Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Let's try the model on *rotated* MNIST","metadata":{"id":"7cvZVi62xADW"}},{"cell_type":"code","source":"import torch.utils.data\nimport torchvision\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import Compose, Pad, ToTensor, Normalize, ToPILImage\nfrom PIL import Image\n\nimport numpy as np\n\n\nclass RotatedMNISTDataset(torch.utils.data.Dataset):\n    '''\n        This class provides MNIST images with random rotations sampled from\n        a list of rotation angles. This list is dependent of the number of tasks\n        `num_tasks` and the distance (measured in degrees) between tasks\n        `per_task_rotation`.\n    '''\n    def __init__(self, root, train=True, transform=None, download=True, num_tasks=5, per_task_rotation=45):\n        self.dataset = torchvision.datasets.MNIST(root=root, train=train, transform=transform, download=download)\n        self.transform = transform\n        self.rotation_angles = []\n        for task in range(num_tasks):\n            self.rotation_angles.append(float((task) * per_task_rotation))\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, label = self.dataset[idx]\n        angle = np.random.choice(self.rotation_angles)  # Randomly choose a rotation angle\n        rotated_image = F.rotate(image, angle, fill=(0,))\n\n\n        return rotated_image, label, angle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:15:58.051269Z","iopub.execute_input":"2024-12-08T13:15:58.051602Z","iopub.status.idle":"2024-12-08T13:15:59.039393Z","shell.execute_reply.started":"2024-12-08T13:15:58.051577Z","shell.execute_reply":"2024-12-08T13:15:59.038746Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def flattened_rotMNIST(num_tasks,\n                       per_task_rotation,\n                       batch_size,\n                       transform=[],\n                       ):\n    '''\n    Returns:\n    - train_loader\n    - test_loader\n    '''\n    g = torch.Generator()\n    g.manual_seed(0)  # Ensure consistent ordering across runs\n\n    # Extend the provided transform with default Pad, ToTensor, and Normalize\n    extended_transform = transform.copy()\n    extended_transform.extend([\n        Pad((0, 0, 1, 1)),  # Add padding\n        ToTensor(),\n        Normalize((0.1307,), (0.3081,))  # Normalize\n    ])\n    transforms = Compose(extended_transform)\n\n    # Create train and test datasets\n    train = RotatedMNISTDataset(\n        root='~/data/', train=True, download=True,\n        transform=transforms, num_tasks=num_tasks, per_task_rotation=per_task_rotation\n    )\n    test = RotatedMNISTDataset(\n        root='~/data/', train=False, download=True,\n        transform=transforms, num_tasks=num_tasks, per_task_rotation=per_task_rotation\n    )\n\n    # Create DataLoaders\n    train_loader = torch.utils.data.DataLoader(\n        train, batch_size=batch_size, shuffle=False,\n        num_workers=0, pin_memory=True, generator=g\n    )\n    test_loader = torch.utils.data.DataLoader(\n        test, batch_size=batch_size, shuffle=True,\n        num_workers=0, pin_memory=True, generator=g\n    )\n\n    return train_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:15:59.041371Z","iopub.execute_input":"2024-12-08T13:15:59.041749Z","iopub.status.idle":"2024-12-08T13:15:59.048382Z","shell.execute_reply.started":"2024-12-08T13:15:59.041723Z","shell.execute_reply":"2024-12-08T13:15:59.047535Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Step 1: Import necessary modules\nimport torch\n\n# Step 2: Set parameters for the new dataset\nnum_tasks = 8  # Number of rotation tasks\nper_task_rotation = 45  # Degrees of rotation per task\nbatch_size = 64  # Batch size for training/testing\n\n# Step 3: Load the rotated MNIST dataset\ntrain_loader, test_loader = flattened_rotMNIST(\n    num_tasks=num_tasks,\n    per_task_rotation=per_task_rotation,\n    batch_size=batch_size\n)\n\nfor images, labels, angles in train_loader:\n    print(f\"Input image shape after transform: {images.shape}\")  # Should be [batch_size, 1, 29, 29]\n    break\n","metadata":{"id":"v2yOYDTgxADW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b937a710-cb29-4369-e1a4-ff0fd7f5fc90","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:15:59.049273Z","iopub.execute_input":"2024-12-08T13:15:59.049487Z","iopub.status.idle":"2024-12-08T13:16:04.551054Z","shell.execute_reply.started":"2024-12-08T13:15:59.049465Z","shell.execute_reply":"2024-12-08T13:16:04.550067Z"}},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 17721602.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /root/data/MNIST/raw/train-images-idx3-ubyte.gz to /root/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 469339.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /root/data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 4415336.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /root/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 3148847.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /root/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/data/MNIST/raw\n\nInput image shape after transform: torch.Size([64, 1, 29, 29])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision.transforms import RandomRotation\nfrom torchvision.transforms import Pad\nfrom torchvision.transforms import Resize\nfrom torchvision.transforms import ToTensor\nfrom torchvision.transforms import Compose\n\nimport numpy as np\n\nfrom PIL import Image\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n","metadata":{"id":"bfQdxQa5xADX","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:16:04.552180Z","iopub.execute_input":"2024-12-08T13:16:04.552441Z","iopub.status.idle":"2024-12-08T13:16:04.557713Z","shell.execute_reply.started":"2024-12-08T13:16:04.552416Z","shell.execute_reply":"2024-12-08T13:16:04.556845Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Build the dataset","metadata":{"id":"Tp027s2axADX"}},{"cell_type":"code","source":"class MnistRotDataset(Dataset):\n\n    def __init__(self, mode, transform=None):\n        assert mode in ['train', 'test']\n\n        if mode == \"train\":\n            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_train_valid.amat\"\n        else:\n            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_test.amat\"\n\n        self.transform = transform\n\n        data = np.loadtxt(file, delimiter=' ')\n\n        self.images = data[:, :-1].reshape(-1, 28, 28).astype(np.float32)\n        self.labels = data[:, -1].astype(np.int64)\n        self.num_samples = len(self.labels)\n\n    def __getitem__(self, index):\n        image, label = self.images[index], self.labels[index]\n        image = Image.fromarray(image)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label\n\n    def __len__(self):\n        return len(self.labels)\n\n# images are padded to have shape 29x29.\n# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\npad = Pad((0, 0, 1, 1), fill=0)\n\n# to reduce interpolation artifacts (e.g. when testing the model on rotated images),\n# we upsample an image by a factor of 3, rotate it and finally downsample it again\nresize1 = Resize(87)\nresize2 = Resize(29)\n\ntotensor = ToTensor()","metadata":{"id":"2bwUWP55xADX","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:16:04.558833Z","iopub.execute_input":"2024-12-08T13:16:04.559135Z","iopub.status.idle":"2024-12-08T13:16:04.574345Z","shell.execute_reply.started":"2024-12-08T13:16:04.559094Z","shell.execute_reply":"2024-12-08T13:16:04.573581Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Let's build the model","metadata":{"id":"3V7y7Y7PxADY"}},{"cell_type":"code","source":"","metadata":{"id":"n_2aTGcrxADY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model is now randomly initialized.\nTherefore, we do not expect it to produce the right class probabilities.\n\nHowever, the model should still produce the same output for rotated versions of the same image.\nThis is true for rotations by multiples of $\\frac{\\pi}{2}$, but is only approximate for rotations by $\\frac{\\pi}{4}$.\n\nLet's test it on a random test image:\nwe feed eight rotated versions of the first image in the test set and print the output logits of the model for each of them.","metadata":{"id":"CgY_kvapxADY"}},{"cell_type":"code","source":"\ndef test_model(model: torch.nn.Module, x: Image):\n    # evaluate the `model` on 8 rotated versions of the input image `x`\n    model.eval()\n\n    wrmup = model(torch.randn(1, 1, 29, 29).to(device))\n    del wrmup\n\n    x = resize1(pad(x))\n\n    print()\n    print('##########################################################################################')\n    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(10)])\n    print(header)\n    with torch.no_grad():\n        for r in range(8):\n            x_transformed = totensor(resize2(x.rotate(r*45., Image.BILINEAR))).reshape(1, 1, 29, 29)\n            x_transformed = x_transformed.to(device)\n\n            y = model(x_transformed)\n            y = y.to('cpu').numpy().squeeze()\n\n            angle = r * 45\n            print(\"{:5d} : {}\".format(angle, y))\n    print('##########################################################################################')\n    print()\n\n","metadata":{"id":"Aj4uO72RxADY","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:16:04.575490Z","iopub.execute_input":"2024-12-08T13:16:04.576001Z","iopub.status.idle":"2024-12-08T13:16:04.589470Z","shell.execute_reply.started":"2024-12-08T13:16:04.575963Z","shell.execute_reply":"2024-12-08T13:16:04.588679Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"id":"dsPbTf7OxADY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"CQTXgSa0xADY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output of the model is already almost invariant.\nHowever, we still observe small fluctuations in the outputs.\n\nThis is because the model contains some operations which might break equivariance.\nFor instance, every convolution includes a padding of $2$ pixels per side. This is adds information about the actual orientation of the grid where the image/feature map is sampled because the padding is not rotated with the image.\n\nDuring training, the model will observe rotated patterns and will learn to ignore the noise coming from the padding.","metadata":{"id":"EfuBMCFPxADY"}},{"cell_type":"markdown","source":"So, let's train the model now.\nThe model is exactly the same used to train a normal *PyTorch* architecture:","metadata":{"id":"6WxgqeXhxADY"}},{"cell_type":"code","source":"loss_function = torch.nn.CrossEntropyLoss()","metadata":{"id":"Vm3B3UAcxADY","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:16:04.590561Z","iopub.execute_input":"2024-12-08T13:16:04.590857Z","iopub.status.idle":"2024-12-08T13:16:04.605195Z","shell.execute_reply.started":"2024-12-08T13:16:04.590815Z","shell.execute_reply":"2024-12-08T13:16:04.604440Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import csv\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\n\n# Create or open a CSV file\ncsv_file = \"/kaggle/working/accuracy_by_percentage.csv\"\nwith open(csv_file, mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Data Percentage\", \"Num Images\", \"Epoch\", \"Accuracy\"])  # CSV header\n\n# Define percentages to sample\npercentages = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\n# Function to create a subset DataLoader for a specific percentage\ndef get_subset_loader(full_loader, percentage):\n    # Get all indices and labels from the existing DataLoader\n    indices = []\n    labels = []\n    for i, (x, t, _) in enumerate(full_loader.dataset):\n        indices.append(i)\n        labels.append(t)\n\n    indices = np.array(indices)\n    labels = np.array(labels)\n\n    # Split indices by label\n    subset_indices = []\n    for label in np.unique(labels):\n        label_indices = indices[labels == label]\n        num_samples = int(len(label_indices) * (percentage / 100))\n        subset_indices.extend(label_indices[:num_samples])\n\n    # Create a subset DataLoader\n    subset_dataset = Subset(full_loader.dataset, subset_indices)\n    subset_loader = DataLoader(subset_dataset, batch_size=full_loader.batch_size, shuffle=True)\n    return subset_loader, len(subset_indices)\n\n# Train the model for different data percentages\nfor percentage in percentages:\n\n    model = C8SteerableCNN().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)\n    \n    # Get the subset DataLoader\n    subset_loader, num_images = get_subset_loader(train_loader, percentage)\n    print(f\"Training with {percentage}% of the data ({num_images} images)\")\n\n    # Training loop\n    for epoch in range(11):  # Number of epochs\n        model.train()\n        for i, (x, t, _) in enumerate(subset_loader):\n            optimizer.zero_grad()\n\n            x = x.to(device)\n            t = t.to(device)\n\n            y = model(x)\n            loss = loss_function(y, t)\n\n            loss.backward()\n            optimizer.step()\n\n        # Testing loop\n        total = 0\n        correct = 0\n        with torch.no_grad():\n            model.eval()\n            for i, (x, t, _) in enumerate(test_loader):\n                x = x.to(device)\n                t = t.to(device)\n\n                y = model(x)\n\n                _, prediction = torch.max(y.data, 1)\n                total += t.shape[0]\n                correct += (prediction == t).sum().item()\n\n        # Calculate accuracy\n        test_accuracy = correct / total * 100.0\n        print(f\"Percentage {percentage}% | Epoch {epoch} | Test Accuracy: {test_accuracy:.2f}%\")\n\n        # Save results to the CSV file\n        with open(csv_file, mode=\"a\", newline=\"\") as file:\n            writer = csv.writer(file)\n            writer.writerow([percentage, num_images, epoch, test_accuracy])\n","metadata":{"id":"Bmr1gRl6xADY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8ea2fb0-5618-47c5-fe7d-8a319ff6fc28","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:16:04.607971Z","iopub.execute_input":"2024-12-08T13:16:04.608254Z","iopub.status.idle":"2024-12-08T13:19:28.246589Z","shell.execute_reply.started":"2024-12-08T13:16:04.608230Z","shell.execute_reply":"2024-12-08T13:19:28.245639Z"}},"outputs":[{"name":"stderr","text":"/kaggle/input/equivarient/e2cnn-master/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:80: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n  full_mask[mask] = norms.to(torch.uint8)\n","output_type":"stream"},{"name":"stdout","text":"Training with 0.2% of the data (113 images)\nPercentage 0.2% | Epoch 0 | Test Accuracy: 8.99%\nPercentage 0.2% | Epoch 1 | Test Accuracy: 10.36%\nPercentage 0.2% | Epoch 2 | Test Accuracy: 12.32%\nPercentage 0.2% | Epoch 3 | Test Accuracy: 23.39%\nPercentage 0.2% | Epoch 4 | Test Accuracy: 33.94%\nPercentage 0.2% | Epoch 5 | Test Accuracy: 38.50%\nPercentage 0.2% | Epoch 6 | Test Accuracy: 43.40%\nPercentage 0.2% | Epoch 7 | Test Accuracy: 48.00%\nPercentage 0.2% | Epoch 8 | Test Accuracy: 50.98%\nPercentage 0.2% | Epoch 9 | Test Accuracy: 53.62%\nPercentage 0.2% | Epoch 10 | Test Accuracy: 56.08%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"###### import matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to display a batch of images\ndef show_images_from_loader(loader, classes, num_images=8):\n    \"\"\"\n    Displays a batch of images from the given DataLoader.\n\n    Args:\n    - loader: The DataLoader to pull images from (e.g., test_loader).\n    - classes: A list of class names corresponding to the dataset labels.\n    - num_images: The number of images to display (default is 8).\n    \"\"\"\n    # Get a single batch from the loader\n    data_iter = iter(loader)\n    images, labels, _ = next(data_iter)\n\n    # Limit to the specified number of images\n    images = images[:num_images]\n    labels = labels[:num_images]\n\n    # Convert from torch tensors to numpy for visualization\n    images = images.numpy()\n\n    # Create a figure\n    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n\n    for i in range(num_images):\n        ax = axes[i]\n        img = np.transpose(images[i], (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n        ax.imshow(img)\n        ax.axis(\"off\")\n        ax.set_title(classes[labels[i]])\n\n    plt.tight_layout()\n    plt.show()\n\n# Example usage\n# Assuming you have a DataLoader named `test_loader` and a `classes` list\n# For example: classes = ['cat', 'dog', 'bird', ...] based on your dataset\nclasses = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']  # Replace with your actual class names\nshow_images_from_loader(test_loader, classes)\n","metadata":{"id":"7dboU8EvDD1p","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"0d29ab05-d70d-4aa6-b28f-0d10243fc00b","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:28.247845Z","iopub.execute_input":"2024-12-08T13:19:28.248210Z","iopub.status.idle":"2024-12-08T13:19:28.490433Z","shell.execute_reply.started":"2024-12-08T13:19:28.248170Z","shell.execute_reply":"2024-12-08T13:19:28.489173Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Assuming you have a DataLoader named `test_loader` and a `classes` list\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# For example: classes = ['cat', 'dog', 'bird', ...] based on your dataset\u001b[39;00m\n\u001b[1;32m     41\u001b[0m classes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace with your actual class names\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mshow_images_from_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 26\u001b[0m, in \u001b[0;36mshow_images_from_loader\u001b[0;34m(loader, classes, num_images)\u001b[0m\n\u001b[1;32m     23\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create a figure\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, num_images, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_images):\n\u001b[1;32m     29\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axes[i]\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}],"execution_count":12}]}